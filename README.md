# ğŸ¬ VideoTranslate AI - Multilingual Video Translation Platform

A sophisticated AI-powered web application that translates video content across multiple languages using advanced machine learning models. The platform leverages Whisper for speech recognition, NLLB-200 for translation, and RAG (Retrieval-Augmented Generation) for context-aware processing.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-3.0-green.svg)

---

## âœ¨ Features

- ğŸ¥ **Video Upload & Processing** - Support for multiple video formats (MP4, AVI, MOV, MKV, WAV)
- ğŸŒ **Multilingual Translation** - Translate videos to and from numerous languages
- ğŸ§  **AI-Powered Intelligence** - Advanced RAG and LLM technology for context-aware translations
- ğŸ“ **Automatic Subtitle Generation** - Generate and embed translated subtitles directly into videos
- âš¡ **Fast Processing** - Efficient algorithms for rapid video translation
- ğŸ” **Content Analysis** - RAG-based relevance checking for accurate translations
- ğŸ’¾ **Easy Download** - Get your translated videos with embedded subtitles

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Backend Framework** | Flask (Python) |
| **Speech Recognition** | OpenAI Whisper |
| **Translation Model** | Facebook NLLB-200 |
| **RAG System** | FAISS + Sentence Transformers |
| **Video Processing** | FFmpeg |
| **Frontend** | HTML5, CSS3, JavaScript, Bootstrap 5 |
| **ML Libraries** | PyTorch, Transformers, HuggingFace |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚  User uploads video
â”‚  (Browser)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Flask API  â”‚  Receives video file
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Whisper   â”‚  Extracts audio & transcribes
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    RAG      â”‚  Context analysis & relevance check
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NLLB-200  â”‚  Translates content
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FFmpeg    â”‚  Embeds subtitles & generates video
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚  Downloads translated video
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python** 3.8 or higher
- **FFmpeg** installed on your system
- **8GB+ RAM** (16GB recommended for optimal performance)
- **5GB+ free disk space** for ML models
- **Internet connection** (for initial model download)

### Installation

#### Option 1: Automated Setup (Recommended)

**macOS / Linux:**
```bash
chmod +x run.sh
./run.sh
```

**Windows:**
```batch
run.bat
```

#### Option 2: Manual Setup

```bash
# Clone the repository
git clone https://github.com/raghulpranxsh/CrossLingualAI.git
cd CrossLingualAI

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Create necessary directories
mkdir -p uploads outputs

# Run the application
python app.py
```

### First Run

On the first run, the application will automatically download required ML models:

- **Whisper Base Model** (~150MB)
- **NLLB-200 Translation Model** (~1.2GB)
- **Sentence Transformers** (~90MB)

**Note:** This initial download may take 10-15 minutes depending on your internet speed. Models are cached locally for subsequent runs.

---

## ğŸ“– Usage

1. **Start the Server**
   ```bash
   python app.py
   ```

2. **Access the Application**
   - Open your browser and navigate to: `http://localhost:5001`

3. **Upload Video**
   - Click "Choose File" or drag and drop your video
   - Supported formats: MP4, AVI, MOV, MKV, WAV
   - Maximum file size: 500MB

4. **Configure Translation**
   - Select source language (or use Auto-detect)
   - Select target language
   - Click "Process Translation"

5. **Download Result**
   - Wait for processing to complete
   - Download your translated video with embedded subtitles

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Main web interface |
| `GET` | `/api/health` | Health check endpoint |
| `POST` | `/api/upload` | Upload and process video |
| `GET` | `/api/download/<filename>` | Download processed video |

### Example API Usage

```bash
# Health check
curl http://localhost:5001/api/health

# Upload video (using curl)
curl -X POST -F "file=@video.mp4" \
     -F "sourceLanguage=auto" \
     -F "targetLanguage=en" \
     http://localhost:5001/api/upload
```

---

## ğŸ§  How It Works

1. **Audio Extraction**: Video file is processed to extract audio track
2. **Speech Recognition**: Whisper transcribes audio to text in the original language
3. **Language Detection**: Automatic detection of source language (if not specified)
4. **RAG Analysis**: Content is analyzed against news corpus for context and relevance
5. **Translation**: NLLB-200 translates the transcribed text to target language
6. **Subtitle Generation**: SRT file is created with translated subtitles and timestamps
7. **Video Processing**: FFmpeg embeds subtitles into the original video
8. **Delivery**: User receives the translated video with embedded subtitles

---

## ğŸ“ Project Structure

```
CrossLingualAI/
â”‚
â”œâ”€â”€ app.py                 # Flask backend server
â”œâ”€â”€ index.html            # Frontend web interface
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ run.sh               # Setup script (macOS/Linux)
â”œâ”€â”€ run.bat              # Setup script (Windows)
â”œâ”€â”€ README.md            # Project documentation
â”‚
â”œâ”€â”€ uploads/             # Temporary upload directory
â”œâ”€â”€ outputs/             # Processed video output directory
â”‚
â”œâ”€â”€ news_corpus_*.json   # News corpus data for RAG
â”œâ”€â”€ faiss_index.index    # FAISS vector index (generated)
â””â”€â”€ docs.json            # Document cache (generated)
```

---

## âš™ï¸ Configuration

### Port Configuration

By default, the server runs on port `5001`. To change this, modify `app.py`:

```python
app.run(debug=True, host='0.0.0.0', port=5001)  # Change port here
```

### Model Configuration

Models are automatically downloaded on first run. To use different Whisper models:

```python
whisper_model = whisper.load_model("base")  # Options: tiny, base, small, medium, large
```

---

## ğŸ› Troubleshooting

### Common Issues

**Port Already in Use**
```bash
# Kill process on port 5001
lsof -ti :5001 | xargs kill -9
```

**FFmpeg Not Found**
```bash
# macOS
brew install ffmpeg

# Linux
sudo apt-get install ffmpeg

# Windows
# Download from https://ffmpeg.org/download.html
```

**Out of Memory**
- Close other applications
- Use smaller Whisper model (tiny/base instead of large)
- Process shorter videos

**Models Not Downloading**
- Check internet connection
- Verify disk space (need ~2GB free)
- Check HuggingFace access

---

## ğŸ“Š Performance

- **Processing Time**: ~2-5 minutes for a 1-minute video
- **Memory Usage**: ~3-4GB during processing
- **Supported Languages**: 20+ languages via NLLB-200
- **Video Formats**: MP4, AVI, MOV, MKV, WAV

---

## ğŸ”’ Security Notes

- Uploaded files are temporarily stored and automatically deleted after processing
- No user data is permanently stored
- All processing happens server-side
- Maximum file size limit: 500MB

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is open-source and available under the MIT License.

---

## ğŸ‘¨â€ğŸ’» Author

**Raghul Pranesh K V**

- ğŸŒ GitHub: [@raghulpranxsh](https://github.com/raghulpranxsh)
- ğŸ’¼ LinkedIn: [raghulpraneshkv](https://www.linkedin.com/in/raghulpraneshkv/)

---

## ğŸ™ Acknowledgments

- OpenAI for Whisper speech recognition model
- Facebook AI for NLLB-200 translation model
- HuggingFace for Transformers and Sentence Transformers
- Flask community for the excellent web framework

---

## ğŸ“§ Contact

For questions, suggestions, or support, please open an issue on GitHub.

---

**Made with â¤ï¸ using AI and Machine Learning**
