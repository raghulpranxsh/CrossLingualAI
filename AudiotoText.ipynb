{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "BUFzhX9ERYpL",
        "outputId": "885bdc1b-03ae-4a98-9cfb-a1967603fe89"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing required packages...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Dependencies installed successfully!\n",
            "Upload a video file to transcribe:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c4d5bf01-7308-4bee-bff2-999b2cafe604\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c4d5bf01-7308-4bee-bff2-999b2cafe604\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Video to Text Converter\n",
        "# Based on https://github.com/Carleslc/AudioToText with updates for video processing\n",
        "\n",
        "# Install necessary dependencies first (for Google Colab)\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Install required packages first\n",
        "print(\"Installing required packages...\")\n",
        "!pip install SpeechRecognition pydub ffmpeg-python -q\n",
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg -qq\n",
        "print(\"Dependencies installed successfully!\")\n",
        "\n",
        "# Now import the required modules\n",
        "import os\n",
        "import speech_recognition as sr\n",
        "import tempfile\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "from google.colab import files\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Extract audio from video file\n",
        "def extract_audio_from_video(video_path, output_audio_path=None):\n",
        "    \"\"\"\n",
        "    Extract audio from video file using ffmpeg\n",
        "    \"\"\"\n",
        "    if output_audio_path is None:\n",
        "        output_audio_path = tempfile.mktemp(suffix='.wav')\n",
        "\n",
        "    cmd = [\n",
        "        'ffmpeg', '-i', video_path,\n",
        "        '-vn',  # No video\n",
        "        '-acodec', 'pcm_s16le',  # Audio codec\n",
        "        '-ar', '44100',  # Sample rate\n",
        "        '-ac', '1',  # Mono\n",
        "        output_audio_path\n",
        "    ]\n",
        "\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_audio_path\n",
        "\n",
        "def get_large_audio_transcription(audio_path, language=\"en-US\", chunk_size_ms=60000):\n",
        "    \"\"\"\n",
        "    Split audio into chunks and apply speech recognition\n",
        "    \"\"\"\n",
        "    r = sr.Recognizer()\n",
        "    sound = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "    # Split audio where silence is detected\n",
        "    chunks = split_on_silence(\n",
        "        sound,\n",
        "        min_silence_len=500,  # minimum silence length in ms\n",
        "        silence_thresh=sound.dBFS-14,  # silence threshold\n",
        "        keep_silence=500  # keep some silence at the beginning and end\n",
        "    )\n",
        "\n",
        "    # If chunks are too few (or silence detection didn't work well), use time-based chunking\n",
        "    if len(chunks) < 2:\n",
        "        chunks = [sound[i:i+chunk_size_ms] for i in range(0, len(sound), chunk_size_ms)]\n",
        "\n",
        "    print(f\"Audio split into {len(chunks)} chunks\")\n",
        "\n",
        "    folder_name = tempfile.mkdtemp()\n",
        "    whole_text = \"\"\n",
        "    timestamps = []\n",
        "    current_time = 0\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
        "        chunk.export(chunk_filename, format=\"wav\")\n",
        "\n",
        "        with sr.AudioFile(chunk_filename) as source:\n",
        "            # Adjust for ambient noise and record\n",
        "            audio_data = r.record(source)\n",
        "\n",
        "            try:\n",
        "                # Use Google Speech Recognition\n",
        "                text = r.recognize_google(audio_data, language=language)\n",
        "                chunk_duration = len(chunk) / 1000.0  # Convert ms to seconds\n",
        "\n",
        "                timestamps.append({\n",
        "                    \"start\": current_time,\n",
        "                    \"end\": current_time + chunk_duration,\n",
        "                    \"text\": text\n",
        "                })\n",
        "\n",
        "                current_time += chunk_duration\n",
        "                whole_text += text + \" \"\n",
        "                print(f\"Chunk {i+1}/{len(chunks)} processed\")\n",
        "\n",
        "            except sr.UnknownValueError:\n",
        "                print(f\"Speech not recognized in chunk {i+1}\")\n",
        "                current_time += len(chunk) / 1000.0\n",
        "            except sr.RequestError as e:\n",
        "                print(f\"API error in chunk {i+1}: {e}\")\n",
        "                current_time += len(chunk) / 1000.0\n",
        "\n",
        "    return whole_text, timestamps\n",
        "\n",
        "def generate_srt(timestamps, filename=\"subtitles.srt\"):\n",
        "    \"\"\"\n",
        "    Generate SRT subtitle file from timestamps\n",
        "    \"\"\"\n",
        "    with open(filename, \"w\") as srt_file:\n",
        "        for i, item in enumerate(timestamps):\n",
        "            start = time.strftime('%H:%M:%S,000', time.gmtime(item[\"start\"]))\n",
        "            end = time.strftime('%H:%M:%S,000', time.gmtime(item[\"end\"]))\n",
        "\n",
        "            srt_file.write(f\"{i+1}\\n\")\n",
        "            srt_file.write(f\"{start} --> {end}\\n\")\n",
        "            srt_file.write(f\"{item['text']}\\n\\n\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "def visualize_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Visualize audio waveform\n",
        "    \"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    samples = np.array(audio.get_array_of_samples())\n",
        "\n",
        "    plt.figure(figsize=(14, 4))\n",
        "    plt.plot(np.linspace(0, len(audio) / 1000, len(samples)), samples)\n",
        "    plt.title('Audio Waveform')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "\n",
        "    # Play audio\n",
        "    display(ipd.Audio(audio_path))\n",
        "\n",
        "# Main function to run the entire process\n",
        "def main():\n",
        "    print(\"Upload a video file to transcribe:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file was uploaded.\")\n",
        "        return\n",
        "\n",
        "    video_filename = list(uploaded.keys())[0]\n",
        "    print(f\"Processing video: {video_filename}\")\n",
        "\n",
        "    # Extract audio from video\n",
        "    audio_path = extract_audio_from_video(video_filename)\n",
        "    print(f\"Audio extracted to: {audio_path}\")\n",
        "\n",
        "    # Visualize the audio\n",
        "    print(\"Audio visualization:\")\n",
        "    visualize_audio(audio_path)\n",
        "\n",
        "    # Select language\n",
        "    print(\"\\nSelect language for transcription:\")\n",
        "    print(\"1. English (en-US)\")\n",
        "    print(\"2. Japanese (ja-JP)\")\n",
        "    print(\"3. Spanish (es-ES)\")\n",
        "    print(\"4. French (fr-FR)\")\n",
        "    print(\"5. German (de-DE)\")\n",
        "\n",
        "    language_codes = {\n",
        "        \"1\": \"en-US\",\n",
        "        \"2\": \"ja-JP\",\n",
        "        \"3\": \"es-ES\",\n",
        "        \"4\": \"fr-FR\",\n",
        "        \"5\": \"de-DE\"\n",
        "    }\n",
        "\n",
        "    choice = input(\"Enter your choice (1-5): \")\n",
        "    language = language_codes.get(choice, \"en-US\")\n",
        "\n",
        "    # Transcribe audio\n",
        "    print(f\"\\nTranscribing audio in {language}...\")\n",
        "    start_time = time.time()\n",
        "    text, timestamps = get_large_audio_transcription(audio_path, language=language)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"\\nTranscription completed in {end_time - start_time:.2f} seconds\")\n",
        "    print(\"\\nTranscribed text:\")\n",
        "    print(text)\n",
        "\n",
        "    # Generate SRT file\n",
        "    srt_filename = generate_srt(timestamps)\n",
        "    print(f\"\\nGenerated SRT file: {srt_filename}\")\n",
        "    files.download(srt_filename)\n",
        "\n",
        "    # Clean up temporary files\n",
        "    os.remove(audio_path)\n",
        "\n",
        "    return text, timestamps, srt_filename\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install necessary packages (for Google Colab)\n",
        "!pip install openai-whisper ffmpeg-python pydub -q\n",
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg -qq\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display\n",
        "\n",
        "# Extract audio from video\n",
        "def extract_audio_from_video(video_path, output_audio_path=None):\n",
        "    if output_audio_path is None:\n",
        "        output_audio_path = tempfile.mktemp(suffix='.wav')\n",
        "\n",
        "    cmd = [\n",
        "        'ffmpeg', '-i', video_path,\n",
        "        '-vn',  # disable video\n",
        "        '-acodec', 'pcm_s16le',\n",
        "        '-ar', '16000',\n",
        "        '-ac', '1',\n",
        "        output_audio_path\n",
        "    ]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_audio_path\n",
        "\n",
        "# Transcribe using Whisper\n",
        "def transcribe_with_whisper(audio_path, language=\"en\"):\n",
        "    model = whisper.load_model(\"base\")  # or \"small\", \"medium\", \"large\"\n",
        "    print(\"Transcribing with Whisper...\")\n",
        "    result = model.transcribe(audio_path, language=language)\n",
        "    return result[\"text\"], result[\"segments\"]\n",
        "\n",
        "# Generate SRT\n",
        "def generate_srt_from_whisper_segments(segments, filename=\"subtitles.srt\"):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for i, segment in enumerate(segments):\n",
        "            start = format_timestamp(segment[\"start\"])\n",
        "            end = format_timestamp(segment[\"end\"])\n",
        "            f.write(f\"{i+1}\\n{start} --> {end}\\n{segment['text']}\\n\\n\")\n",
        "    return filename\n",
        "\n",
        "# Format seconds to SRT time format\n",
        "def format_timestamp(seconds):\n",
        "    hrs = int(seconds // 3600)\n",
        "    mins = int((seconds % 3600) // 60)\n",
        "    secs = int(seconds % 60)\n",
        "    millis = int((seconds - int(seconds)) * 1000)\n",
        "    return f\"{hrs:02}:{mins:02}:{secs:02},{millis:03}\"\n",
        "\n",
        "# Visualize Audio\n",
        "def visualize_audio(audio_path):\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    samples = np.array(audio.get_array_of_samples())\n",
        "    plt.figure(figsize=(14, 4))\n",
        "    plt.plot(np.linspace(0, len(audio) / 1000, len(samples)), samples)\n",
        "    plt.title('Audio Waveform')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "    display(ipd.Audio(audio_path))\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    print(\"Upload a video file to transcribe:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "\n",
        "    video_filename = list(uploaded.keys())[0]\n",
        "    print(f\"Processing video: {video_filename}\")\n",
        "\n",
        "    audio_path = extract_audio_from_video(video_filename)\n",
        "    print(f\"Extracted audio: {audio_path}\")\n",
        "\n",
        "    visualize_audio(audio_path)\n",
        "\n",
        "    print(\"\\nSelect language:\")\n",
        "    print(\"1. English\\n2. Japanese\\n3. Spanish\\n4. French\\n5. German\")\n",
        "    lang_map = {\n",
        "        \"1\": \"en\", \"2\": \"ja\", \"3\": \"es\", \"4\": \"fr\", \"5\": \"de\"\n",
        "    }\n",
        "    choice = input(\"Enter choice (1-5): \")\n",
        "    language = lang_map.get(choice, \"en\")\n",
        "\n",
        "    text, segments = transcribe_with_whisper(audio_path, language=language)\n",
        "    print(\"\\nTranscription complete:\")\n",
        "    print(text)\n",
        "\n",
        "    srt_filename = generate_srt_from_whisper_segments(segments)\n",
        "    print(f\"\\nGenerated subtitle file: {srt_filename}\")\n",
        "    files.download(srt_filename)\n",
        "\n",
        "    os.remove(audio_path)\n",
        "    return text, segments, srt_filename\n",
        "\n",
        "# Run the main\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NnrQHr-3eynI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install openai-whisper ffmpeg-python pydub -q\n",
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg -qq\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display\n",
        "\n",
        "# Extract audio from video\n",
        "def extract_audio_from_video(video_path, output_audio_path=None):\n",
        "    if output_audio_path is None:\n",
        "        output_audio_path = tempfile.mktemp(suffix='.wav')\n",
        "    cmd = [\n",
        "        'ffmpeg', '-i', video_path,\n",
        "        '-vn',  # no video\n",
        "        '-acodec', 'pcm_s16le',\n",
        "        '-ar', '16000',\n",
        "        '-ac', '1',\n",
        "        output_audio_path\n",
        "    ]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_audio_path\n",
        "\n",
        "# Format timestamp for SRT\n",
        "def format_timestamp(seconds):\n",
        "    hrs = int(seconds // 3600)\n",
        "    mins = int((seconds % 3600) // 60)\n",
        "    secs = int(seconds % 60)\n",
        "    millis = int((seconds - int(seconds)) * 1000)\n",
        "    return f\"{hrs:02}:{mins:02}:{secs:02},{millis:03}\"\n",
        "\n",
        "# Generate SRT file\n",
        "def generate_srt_from_whisper_segments(segments, filename=\"subtitles.srt\"):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for i, segment in enumerate(segments):\n",
        "            start = format_timestamp(segment[\"start\"])\n",
        "            end = format_timestamp(segment[\"end\"])\n",
        "            f.write(f\"{i+1}\\n{start} --> {end}\\n{segment['text']}\\n\\n\")\n",
        "    return filename\n",
        "\n",
        "# Visualize Audio\n",
        "def visualize_audio(audio_path):\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    samples = np.array(audio.get_array_of_samples())\n",
        "    plt.figure(figsize=(14, 4))\n",
        "    plt.plot(np.linspace(0, len(audio) / 1000, len(samples)), samples)\n",
        "    plt.title('Audio Waveform')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "    display(ipd.Audio(audio_path))\n",
        "\n",
        "# Transcribe with Whisper\n",
        "def transcribe_with_whisper(audio_path, language=\"en\"):\n",
        "    model = whisper.load_model(\"base\")  # Change to \"medium\"/\"large\" if needed\n",
        "    print(\"Transcribing...\")\n",
        "    result = model.transcribe(audio_path, language=language)\n",
        "    return result[\"text\"], result[\"segments\"]\n",
        "\n",
        "# Burn subtitles into video using ffmpeg\n",
        "def burn_subtitles_into_video(video_path, srt_path, output_path=None):\n",
        "    if output_path is None:\n",
        "        output_path = tempfile.mktemp(suffix='.mp4')\n",
        "    cmd = [\n",
        "        'ffmpeg', '-i', video_path,\n",
        "        '-vf', f\"subtitles={srt_path}\",\n",
        "        '-c:a', 'copy',\n",
        "        output_path\n",
        "    ]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_path\n",
        "\n",
        "# Main flow\n",
        "def main():\n",
        "    print(\"Upload a video file:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "\n",
        "    video_filename = list(uploaded.keys())[0]\n",
        "    print(f\"Processing: {video_filename}\")\n",
        "\n",
        "    audio_path = extract_audio_from_video(video_filename)\n",
        "    print(f\"Extracted audio to: {audio_path}\")\n",
        "    visualize_audio(audio_path)\n",
        "\n",
        "    print(\"\\nChoose Language:\")\n",
        "    print(\"1. English\\n2. Japanese\\n3. Spanish\\n4. French\\n5. German\")\n",
        "    lang_map = {\n",
        "        \"1\": \"en\", \"2\": \"ja\", \"3\": \"es\", \"4\": \"fr\", \"5\": \"de\"\n",
        "    }\n",
        "    choice = input(\"Enter choice (1-5): \")\n",
        "    language = lang_map.get(choice, \"en\")\n",
        "\n",
        "    text, segments = transcribe_with_whisper(audio_path, language=language)\n",
        "    print(\"\\nTranscription:\")\n",
        "    print(text)\n",
        "\n",
        "    srt_path = generate_srt_from_whisper_segments(segments)\n",
        "    print(f\"\\nSRT file created: {srt_path}\")\n",
        "\n",
        "    output_video_path = burn_subtitles_into_video(video_filename, srt_path)\n",
        "    print(f\"\\nVideo with captions created: {output_video_path}\")\n",
        "\n",
        "    files.download(output_video_path)\n",
        "    os.remove(audio_path)\n",
        "    return output_video_path\n",
        "\n",
        "# Run the main\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ZcWsq7298UcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper ffmpeg-python pydub argostranslate -q\n",
        "!apt-get install -y ffmpeg -qq\n"
      ],
      "metadata": {
        "id": "wmIKwjThHaIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "from argostranslate import package, translate\n",
        "\n",
        "# Setup Argos Translate\n",
        "def setup_translation(source_lang_code, target_lang_code):\n",
        "    if source_lang_code == target_lang_code:\n",
        "        return None\n",
        "\n",
        "    available_packages = package.get_available_packages()\n",
        "    matching = next(\n",
        "        (p for p in available_packages if p.from_code == source_lang_code and p.to_code == target_lang_code),\n",
        "        None\n",
        "    )\n",
        "\n",
        "    if matching:\n",
        "        download_path = matching.download()\n",
        "        package.install_from_path(download_path)\n",
        "\n",
        "    installed_languages = package.get_installed_packages()\n",
        "    for p in installed_languages:\n",
        "        if p.from_code == source_lang_code and p.to_code == target_lang_code:\n",
        "            return p.get_translation()\n",
        "\n",
        "    raise Exception(f\"No translator found for {source_lang_code} ‚Üí {target_lang_code}\")\n",
        "\n",
        "# Extract audio from video\n",
        "def extract_audio_from_video(video_path):\n",
        "    output_audio_path = tempfile.mktemp(suffix='.wav')\n",
        "    cmd = ['ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', output_audio_path]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_audio_path\n",
        "\n",
        "# Transcribe audio using Whisper\n",
        "def transcribe_with_whisper(audio_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"], result[\"segments\"]\n",
        "\n",
        "# Format timestamp for SRT\n",
        "def format_timestamp(seconds):\n",
        "    hrs = int(seconds // 3600)\n",
        "    mins = int((seconds % 3600) // 60)\n",
        "    secs = int(seconds % 60)\n",
        "    millis = int((seconds - int(seconds)) * 1000)\n",
        "    return f\"{hrs:02}:{mins:02}:{secs:02},{millis:03}\"\n",
        "\n",
        "# Generate translated SRT\n",
        "def generate_translated_srt(segments, translator, filename=\"translated_subtitles.srt\"):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for i, segment in enumerate(segments):\n",
        "            start = format_timestamp(segment[\"start\"])\n",
        "            end = format_timestamp(segment[\"end\"])\n",
        "            if translator:\n",
        "                translated_text = translator.translate(segment[\"text\"])\n",
        "            else:\n",
        "                translated_text = segment[\"text\"]\n",
        "            f.write(f\"{i+1}\\n{start} --> {end}\\n{translated_text}\\n\\n\")\n",
        "    return filename\n",
        "\n",
        "# Main\n",
        "def main():\n",
        "    print(\"Upload a video file to transcribe and translate:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "\n",
        "    video_filename = list(uploaded.keys())[0]\n",
        "    audio_path = extract_audio_from_video(video_filename)\n",
        "\n",
        "    text, segments = transcribe_with_whisper(audio_path)\n",
        "\n",
        "    print(\"\\nChoose target language:\")\n",
        "    print(\"1. English\\n2. Japanese\\n3. Spanish\\n4. French\\n5. German\")\n",
        "    lang_map = {\"1\": \"en\", \"2\": \"ja\", \"3\": \"es\", \"4\": \"fr\", \"5\": \"de\"}\n",
        "    choice = input(\"Enter choice (1-5): \")\n",
        "    target_lang = lang_map.get(choice, \"en\")\n",
        "\n",
        "    translator = setup_translation(\"en\", target_lang)\n",
        "\n",
        "    # Generate and save translated subtitles\n",
        "    srt_file = generate_translated_srt(segments, translator)\n",
        "    print(f\"Subtitles saved as: {srt_file}\")\n",
        "\n",
        "    # Download the file\n",
        "    files.download(srt_file)\n",
        "\n",
        "    os.remove(audio_path)\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "KV0KO0FyQGNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install pydub\n",
        "!pip install requests\n",
        "!sudo apt-get install ffmpeg"
      ],
      "metadata": {
        "id": "vL_CdX6FbXwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import whisper\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Extract audio from video\n",
        "def extract_audio_from_video(video_path):\n",
        "    output_audio_path = tempfile.mktemp(suffix='.wav')\n",
        "    cmd = ['ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', output_audio_path]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_audio_path\n",
        "\n",
        "# Step 2: Transcribe with Whisper (auto language detection)\n",
        "def transcribe_with_whisper(audio_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"], result[\"language\"]\n",
        "\n",
        "# Step 3: Main\n",
        "def main():\n",
        "    print(\"Upload a video file:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "\n",
        "    video_filename = list(uploaded.keys())[0]\n",
        "    audio_path = extract_audio_from_video(video_filename)\n",
        "\n",
        "    text, detected_lang = transcribe_with_whisper(audio_path)\n",
        "\n",
        "    print(f\"\\n‚úÖ Detected Language: {detected_lang}\")\n",
        "    print(\"\\nüìù Transcription in Original Language:\\n\")\n",
        "    print(text)\n",
        "\n",
        "    os.remove(audio_path)\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "1CquipzTbhny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai-whisper transformers sentencepiece ffmpeg-python"
      ],
      "metadata": {
        "id": "W7CdMFNciYGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "import subprocess\n",
        "import whisper\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from google.colab import files  # Only if using Google Colab\n",
        "\n",
        "# Whisper to NLLB mapping\n",
        "WHISPER_TO_NLLB = {\n",
        "    \"en\": \"eng_Latn\",\n",
        "    \"es\": \"spa_Latn\",\n",
        "    \"fr\": \"fra_Latn\",\n",
        "    \"de\": \"deu_Latn\",\n",
        "    \"ja\": \"jpn_Jpan\",\n",
        "    \"hi\": \"hin_Deva\",\n",
        "    \"ta\": \"tam_Taml\",\n",
        "    \"te\": \"tel_Telu\",\n",
        "    \"zh\": \"zho_Hans\",\n",
        "    \"ko\": \"kor_Hang\",\n",
        "}\n",
        "\n",
        "def extract_audio_from_video(video_path):\n",
        "    output_audio_path = tempfile.mktemp(suffix=\".wav\")\n",
        "    cmd = ['ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', output_audio_path]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_audio_path\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"], result[\"language\"]\n",
        "\n",
        "def translate_to_english(native_text, whisper_lang_code):\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    nllb_lang_code = WHISPER_TO_NLLB.get(whisper_lang_code)\n",
        "    if not nllb_lang_code:\n",
        "        raise ValueError(f\"Unsupported language code: {whisper_lang_code}\")\n",
        "\n",
        "    # Get forced_bos_token_id safely\n",
        "    tokenizer.src_lang = nllb_lang_code\n",
        "    tokenizer.tgt_lang = \"eng_Latn\"\n",
        "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tgt_lang)\n",
        "\n",
        "    # Split long text into smaller chunks\n",
        "    sentences = native_text.strip().split(\"„ÄÇ\")  # Japanese sentence end\n",
        "    chunks = [s.strip() + \"„ÄÇ\" for s in sentences if s.strip()]\n",
        "\n",
        "    translated_parts = []\n",
        "    for chunk in chunks:\n",
        "        try:\n",
        "            inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True,\n",
        "                               truncation=True, max_length=512)\n",
        "            translated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                forced_bos_token_id=forced_bos_token_id,\n",
        "                max_length=512,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            translated_text = tokenizer.decode(translated_tokens[0],\n",
        "                                               skip_special_tokens=True)\n",
        "            translated_parts.append(translated_text)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error translating chunk: {chunk[:30]}... -> {e}\")\n",
        "\n",
        "    return \" \".join(translated_parts)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"üìÇ Please upload a video file (.mp4, .mkv, etc.):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "\n",
        "    print(\"\\nüîä Extracting audio...\")\n",
        "    audio_path = extract_audio_from_video(video_path)\n",
        "\n",
        "    print(\"üìù Transcribing with Whisper...\")\n",
        "    native_text, detected_lang = transcribe_audio(audio_path)\n",
        "    print(f\"\\nüåç Detected Language: {detected_lang}\")\n",
        "    print(f\"\\nüó£Ô∏è Native Transcription:\\n{native_text.strip()}\")\n",
        "\n",
        "    print(\"\\nüåê Translating to English...\")\n",
        "    translated_text = translate_to_english(native_text, detected_lang)\n",
        "    print(f\"\\n‚úÖ English Translation:\\n{translated_text.strip()}\")\n",
        "\n",
        "    os.remove(audio_path)\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "OF0wPwHaicBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q sentence-transformers faiss-cpu transformers whisper"
      ],
      "metadata": {
        "id": "hRhTklF-8OfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y whisper\n"
      ],
      "metadata": {
        "id": "9bMS_oC1_Loy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper\n"
      ],
      "metadata": {
        "id": "5fPDGCxY_Pcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "import subprocess\n",
        "import whisper\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files, drive\n",
        "\n",
        "# Whisper to NLLB language code mapping\n",
        "WHISPER_TO_NLLB = {\n",
        "    \"en\": \"eng_Latn\",\n",
        "    \"es\": \"spa_Latn\",\n",
        "    \"fr\": \"fra_Latn\",\n",
        "    \"de\": \"deu_Latn\",\n",
        "    \"ja\": \"jpn_Jpan\",\n",
        "    \"hi\": \"hin_Deva\",\n",
        "    \"ta\": \"tam_Taml\",\n",
        "    \"te\": \"tel_Telu\",\n",
        "    \"zh\": \"zho_Hans\",\n",
        "    \"ko\": \"kor_Hang\",\n",
        "}\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths for FAISS index and documents\n",
        "docs_path = \"/content/drive/MyDrive/news_json/sample_news.json\"\n",
        "index_path = \"/content/drive/MyDrive/news_json/faiss_index.index\"\n",
        "\n",
        "\n",
        "# List contents of the folder to confirm\n",
        "folder = \"/content/drive/MyDrive/news_json\"\n",
        "print(\"üìÅ Files in your folder:\", os.listdir(folder))\n",
        "\n",
        "# Step 2: Load or Create FAISS Index\n",
        "def load_or_create_faiss_index(docs_path, model, index_path=\"faiss_index.index\"):\n",
        "    if os.path.exists(index_path) and os.path.exists(\"docs.json\"):\n",
        "        print(\"üì¶ Loading existing FAISS index and documents...\")\n",
        "        faiss_index = faiss.read_index(index_path)\n",
        "        with open(\"docs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "            documents = json.load(f)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è FAISS index or documents not found. Creating new index...\")\n",
        "\n",
        "        # ‚úÖ Read and extract content from your articles\n",
        "        with open(docs_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "            articles = data.get(\"articles\", [])\n",
        "            documents = []\n",
        "            for article in articles:\n",
        "                content = article.get(\"content\")\n",
        "                if content:  # only add non-empty content\n",
        "                    documents.append(content)\n",
        "\n",
        "        if not documents:\n",
        "            raise ValueError(\"No valid 'content' fields found in the JSON file.\")\n",
        "\n",
        "        # ‚úÖ Generate embeddings\n",
        "        doc_embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "        # ‚úÖ Build FAISS index\n",
        "        faiss_index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
        "        faiss_index.add(doc_embeddings)\n",
        "\n",
        "        # ‚úÖ Save index and documents\n",
        "        faiss.write_index(faiss_index, index_path)\n",
        "        with open(\"docs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(documents, f)\n",
        "\n",
        "    return faiss_index, documents\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Extract Audio\n",
        "def extract_audio_from_video(video_path):\n",
        "    output_audio_path = tempfile.mktemp(suffix=\".wav\")\n",
        "    cmd = ['ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', output_audio_path]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_audio_path\n",
        "\n",
        "# Step 4: Whisper Transcription\n",
        "def transcribe_audio(audio_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"], result[\"language\"]\n",
        "\n",
        "# Step 5: Translate to English\n",
        "def translate_to_english(native_text, whisper_lang_code):\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    nllb_lang_code = WHISPER_TO_NLLB.get(whisper_lang_code)\n",
        "    if not nllb_lang_code:\n",
        "        raise ValueError(f\"Unsupported language code: {whisper_lang_code}\")\n",
        "\n",
        "    tokenizer.src_lang = nllb_lang_code\n",
        "    tokenizer.tgt_lang = \"eng_Latn\"\n",
        "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tgt_lang)\n",
        "\n",
        "    sentences = native_text.strip().split(\"„ÄÇ\")\n",
        "    chunks = [s.strip() + \"„ÄÇ\" for s in sentences if s.strip()]\n",
        "\n",
        "    translated_parts = []\n",
        "    for chunk in chunks:\n",
        "        try:\n",
        "            inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True,\n",
        "                               truncation=True, max_length=512)\n",
        "            translated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                forced_bos_token_id=forced_bos_token_id,\n",
        "                max_length=512,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "            translated_parts.append(translated_text)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error translating chunk: {chunk[:30]}... -> {e}\")\n",
        "\n",
        "    return \" \".join(translated_parts)\n",
        "\n",
        "# Step 6: RAG Relevancy Check\n",
        "def is_relevant(text, index, documents, threshold=0.5):\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    query_embedding = model.encode([text])\n",
        "    D, I = index.search(query_embedding, k=1)\n",
        "    matched_doc = documents[I[0][0]]\n",
        "    score = cosine_similarity(query_embedding, model.encode([matched_doc]))[0][0]\n",
        "\n",
        "    return score >= threshold, documents[I[0][0]]\n",
        "\n",
        "# Step 7: Generate SRT file\n",
        "def generate_srt(translated_text):\n",
        "    srt_path = tempfile.mktemp(suffix=\".srt\")\n",
        "    lines = translated_text.split('. ')\n",
        "    with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(lines, 1):\n",
        "            start = f\"00:00:{(i-1)*5:02},000\"\n",
        "            end = f\"00:00:{i*5:02},000\"\n",
        "            f.write(f\"{i}\\n{start} --> {end}\\n{line.strip()}\\n\\n\")\n",
        "    return srt_path\n",
        "\n",
        "# Step 8: Embed subtitle into video\n",
        "def embed_subtitle(video_path, srt_path):\n",
        "    output_video = video_path.rsplit('.', 1)[0] + \"_subtitled.mp4\"\n",
        "    cmd = ['ffmpeg', '-i', video_path, '-vf', f\"subtitles={srt_path}\", '-c:a', 'copy', output_video]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_video\n",
        "\n",
        "def main():\n",
        "    print(\"üìÇ Please upload a video file (.mp4, .mkv, etc.):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "\n",
        "    print(\"\\nüîä Extracting audio...\")\n",
        "    audio_path = extract_audio_from_video(video_path)\n",
        "\n",
        "    print(\"üìù Transcribing with Whisper...\")\n",
        "    native_text, detected_lang = transcribe_audio(audio_path)\n",
        "    print(f\"\\nüåç Detected Language: {detected_lang}\")\n",
        "    print(f\"\\nüó£Ô∏è Native Transcription:\\n{native_text.strip()}\")\n",
        "\n",
        "    print(\"\\nüåê Translating to English...\")\n",
        "    translated_text = translate_to_english(native_text, detected_lang)\n",
        "    print(f\"\\n‚úÖ English Translation:\\n{translated_text.strip()}\")\n",
        "\n",
        "    print(\"\\nüì¶ Loading RAG index...\")\n",
        "    sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    index, documents = load_or_create_faiss_index(docs_path, sentence_model, index_path=index_path)\n",
        "\n",
        "    print(\"\\nüîç Checking relevancy with RAG...\")\n",
        "    relevant, doc = is_relevant(translated_text, index, documents)\n",
        "    if relevant:\n",
        "        print(\"‚úÖ Content is relevant to the RAG context.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Content is NOT relevant to the RAG context. Skipping subtitle embedding.\")\n",
        "\n",
        "    print(\"\\nüìù Generating subtitle file...\")\n",
        "    srt_path = generate_srt(translated_text)\n",
        "\n",
        "    print(\"üéûÔ∏è Embedding subtitles into video...\")\n",
        "    output_video = embed_subtitle(video_path, srt_path)\n",
        "\n",
        "    print(f\"\\n‚úÖ Done! Subtitled video saved as: {output_video}\")\n",
        "\n",
        "    # Optionally offer download\n",
        "    files.download(output_video)\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "yQp45Jlv885F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio==4.18.0\n",
        "!pip install gradio==4.18.0 ffmpeg-python openai-whisper sentence-transformers faiss-cpu transformers\n",
        "!pip install \"pydantic<2.0\"\n"
      ],
      "metadata": {
        "id": "ccVSJmkhcyR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import tempfile\n",
        "import subprocess\n",
        "import whisper\n",
        "import json\n",
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Language Mapping\n",
        "WHISPER_TO_NLLB = {\n",
        "    \"en\": \"eng_Latn\", \"es\": \"spa_Latn\", \"fr\": \"fra_Latn\", \"de\": \"deu_Latn\",\n",
        "    \"ja\": \"jpn_Jpan\", \"hi\": \"hin_Deva\", \"ta\": \"tam_Taml\", \"te\": \"tel_Telu\",\n",
        "    \"zh\": \"zho_Hans\", \"ko\": \"kor_Hang\"\n",
        "}\n",
        "\n",
        "# RAG index loading\n",
        "docs_path = \"/content/sample_news.json\"  # update your file path\n",
        "index_path = \"/content/faiss_index.index\"\n",
        "\n",
        "def load_or_create_faiss_index(model):\n",
        "    if os.path.exists(index_path) and os.path.exists(\"docs.json\"):\n",
        "        faiss_index = faiss.read_index(index_path)\n",
        "        with open(\"docs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "            documents = json.load(f)\n",
        "    else:\n",
        "        with open(docs_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "            documents = [a.get(\"content\") for a in data.get(\"articles\", []) if a.get(\"content\")]\n",
        "        doc_embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "        faiss_index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
        "        faiss_index.add(doc_embeddings)\n",
        "        faiss.write_index(faiss_index, index_path)\n",
        "        with open(\"docs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(documents, f)\n",
        "    return faiss_index, documents\n",
        "\n",
        "def extract_audio(video_path):\n",
        "    audio_path = tempfile.mktemp(suffix=\".wav\")\n",
        "    cmd = ['ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', audio_path]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return audio_path\n",
        "\n",
        "def transcribe(audio_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"], result[\"language\"]\n",
        "\n",
        "def translate_to_english(text, lang):\n",
        "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    src_lang = WHISPER_TO_NLLB.get(lang)\n",
        "    if not src_lang:\n",
        "        raise ValueError(f\"Unsupported language code: {lang}\")\n",
        "\n",
        "    tokenizer.src_lang = src_lang\n",
        "    tokenizer.tgt_lang = \"eng_Latn\"\n",
        "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tgt_lang)\n",
        "\n",
        "    sentences = text.strip().split(\"„ÄÇ\")\n",
        "    chunks = [s.strip() + \"„ÄÇ\" for s in sentences if s.strip()]\n",
        "    translated = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, max_length=512)\n",
        "        translated.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "    return \" \".join(translated)\n",
        "\n",
        "def generate_srt(text):\n",
        "    srt_path = tempfile.mktemp(suffix=\".srt\")\n",
        "    lines = text.split('. ')\n",
        "    with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(lines, 1):\n",
        "            start = f\"00:00:{(i-1)*5:02},000\"\n",
        "            end = f\"00:00:{i*5:02},000\"\n",
        "            f.write(f\"{i}\\n{start} --> {end}\\n{line.strip()}\\n\\n\")\n",
        "    return srt_path\n",
        "\n",
        "def embed_subtitle(video_path, srt_path):\n",
        "    output_video = tempfile.mktemp(suffix=\".mp4\")\n",
        "    cmd = ['ffmpeg', '-i', video_path, '-vf', f\"subtitles={srt_path}\", '-c:a', 'copy', output_video]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    return output_video\n",
        "\n",
        "def process_video(video):\n",
        "    # Save uploaded video\n",
        "    video_path = tempfile.mktemp(suffix=\".mp4\")\n",
        "    with open(video_path, \"wb\") as f:\n",
        "        f.write(video.read())\n",
        "\n",
        "    # Step-by-step\n",
        "    audio = extract_audio(video_path)\n",
        "    native_text, lang = transcribe(audio)\n",
        "    translated = translate_to_english(native_text, lang)\n",
        "\n",
        "    rag_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    index, docs = load_or_create_faiss_index(rag_model)\n",
        "    query_embedding = rag_model.encode([translated])\n",
        "    D, I = index.search(query_embedding, k=1)\n",
        "    matched_doc = docs[I[0][0]]\n",
        "    similarity = cosine_similarity(query_embedding, rag_model.encode([matched_doc]))[0][0]\n",
        "\n",
        "    if similarity < 0.5:\n",
        "        return \"‚ùå The video content is not relevant to the RAG context.\", None\n",
        "\n",
        "    srt_path = generate_srt(translated)\n",
        "    result_video = embed_subtitle(video_path, srt_path)\n",
        "    return \"‚úÖ Video translated and subtitled!\", result_video\n",
        "\n",
        "# Gradio UI\n",
        "demo = gr.Interface(\n",
        "    fn=process_video,\n",
        "    inputs=gr.File(label=\"Upload Video\"),\n",
        "    outputs=[gr.Text(label=\"Status\"), gr.Video(label=\"Subtitled Video\")],\n",
        "    title=\"üìΩÔ∏è Subtitle Translator\",\n",
        "    description=\"Upload a video and get back a subtitled version translated to English using Whisper and NLLB.\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True, share=True)\n"
      ],
      "metadata": {
        "id": "CzGHJMrsb-Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYtmZV7Yb_fV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}